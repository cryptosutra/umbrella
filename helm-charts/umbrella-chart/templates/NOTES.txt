1. Get the Airflow application URL by running these commands:
{{- if .Values.networking.ingress.enabled }}
{{- range $host := .Values.networking.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.networking.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
  {{- end }}
{{- end }}
{{- else if contains "NodePort" .Values.airflow.webserver.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "enterprise-data-platform.fullname" . }}-airflow-webserver)
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.airflow.webserver.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "enterprise-data-platform.fullname" . }}-airflow-webserver'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "enterprise-data-platform.fullname" . }}-airflow-webserver --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.airflow.webserver.service.port }}
{{- else if contains "ClusterIP" .Values.airflow.webserver.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "enterprise-data-platform.name" . }}-airflow,app.kubernetes.io/instance={{ .Release.Name }},component=webserver" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}

2. Login to Airflow using:
   Username: admin
   Password: admin

3. Components deployed:
   - PostgreSQL: Database for Airflow metadata
   - Apache Spark: Distributed computing engine with Kubernetes execution
   - Apache Airflow: Workflow orchestration platform

4. Tenant Configuration:
   - Tenant Name: {{ .Values.global.tenant.name }}
   - Namespace: {{ .Values.global.tenant.namespace }}
   - Image Registry: {{ .Values.global.imageRegistry.url }}

5. To create Spark jobs in Airflow, use the Spark connection configured as 'spark_k8s'

6. For custom DAGs, place them in the mounted DAGs volume or configure your CI/CD to deploy them.